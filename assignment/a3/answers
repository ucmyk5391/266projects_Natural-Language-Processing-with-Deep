# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 3 parts for a total of 100.0 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Machine Translation with T5 (82 points)
# - Summarization Test (7.5 points)
# - Summarization LLM Test (6.5 points)
# - Correct submission (2 point)
# - Answer file parses (2 point)



###################################################################
###################################################################
## Machine Translation with T5 (82 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (1): Experiment with model dimensions (26 points)  | 
# ------------------------------------------------------------------

# Question 1.a (/5): What is the final validation loss that you were able to achieve for the part1 model after training for 30 epochs?
machine_translation_with_t5_1_1_a: 
- 2.02044

# Question 1.b (/4): Which model config parameters (if any) did you increase, to achieve a lower validation loss, while staying within the training time and overfitting guidelines?
machine_translation_with_t5_1_1_b: 
- num_layers

# Question 1.c (/4): Which model config parameters (if any) did you decrease, to achieve a lower validation loss, while staying within the training time and overfitting guidelines?
machine_translation_with_t5_1_1_c: 
- embed_dim, dense_dim, num_heads

# Question 1.d (/3): What seems to be particularly bad about the part1 model's translations?
# (This question is multiple choice.  Delete all but the correct answer).
machine_translation_with_t5_1_1_d: 
 - The model keeps repeating the same common words or phrases over and over, which don't produce very meaningful statements.

# Question 1.e (/4): Which .generate() parameter seemed to help the most in addressing the main shortcoming(s) that you noticed in the part1 model's output?
# (This question is multiple choice.  Delete all but the correct answer).
machine_translation_with_t5_1_1_e: 
 - no_repeat_ngram_size

# Question 1.f (/3): What is the overall BLEU score that you achieved on the test set for the part1 model?
machine_translation_with_t5_1_1_f: 
- 0.01247

# Question 1.g (/3): What is the mean BLEURT score that you achieved on the test set for the part1 model?
machine_translation_with_t5_1_1_g: 
- -1.1468


# ------------------------------------------------------------------
# | Section (2): Small Pre-trained T5 Model (42 points)  | 
# ------------------------------------------------------------------

# Question 2.a (/5): What is the final validation loss that you were able to achieve for the part2 model after training for 4 epochs?
machine_translation_with_t5_2_2_a: 
- 0.67744

# Question 2.b (/4): What is the overall BLEU score that you achieved on the test set for the part2 model?
machine_translation_with_t5_2_2_b: 
- 0.33669

# Question 2.c (/4): What is the mean BLEURT score that you achieved on the test set for the part2 model?
machine_translation_with_t5_2_2_c: 
- 0.025577

# Question 2.d (/4): What do you notice about the part2 model's output? It should be much better than the part1 model's output. But the translations still probably don't perfectly match the reference human translations. What does the part2 model seem to still be doing poorly?
# (This question is multiple choice.  Delete all but the correct answer).
machine_translation_with_t5_2_2_d: 
 - The generated translations are too similar to the input text, and haven't been rephrased as much as the reference human translations.

# Question 2.e (/4): What is the modern style classifier score that you got for the part2 model's generated translations?
machine_translation_with_t5_2_2_e: 
- 0.48887

# Question 2.f (/4): What is the modern style classifier score that you got for the human reference translations?
machine_translation_with_t5_2_2_f: 
- 0.82067

# Question 2.g (/4): What is the modern style classifier score that you got for the original Shakespeare text?
machine_translation_with_t5_2_2_g: 
- 0.31256

# Question 2.h (/3): What do you notice about differences between these scores, and what does that tell you about what the part2 model is doing?
# (This question is multiple choice.  Delete all but the correct answer).
machine_translation_with_t5_2_2_h: 
 - The part2 model is generating output that is partly modernized, more modern than the original Shakespeare, but still not as modern as the human references.

# Question 2.i (/3): Which decoder strategy seemed to increase the modern style score the most?
# (This question is multiple choice.  Delete all but the correct answer).
machine_translation_with_t5_2_2_i: 
 - Using a looser sampling method to allow the model to choose more varied output (e.g. top-k or top-p rather than beam search, especially with higher k or p and/or higher temperature).

# Question 2.j (/3): What happens to the other evaluation metrics when you try to increase the modern style score by varying the decoder strategy discussed in 2.i?
# (This question is multiple choice.  Delete all but the correct answer).
machine_translation_with_t5_2_2_j: 
 - BLEU and BLEURT both seem to be negatively correlated with the modern style score, when changing the decoder strategy.

# Question 2.k (/4): Why do you think the relationship in question 2.j is happening?
# (This question is multiple choice.  Delete all but the correct answer).
machine_translation_with_t5_2_2_k: 
 - A looser decoder strategy gives the model more freedom to choose more modern style words, which the pre-trained model is more familiar with, but that freedom can make the model less likely to end up with the exact same words or meaning as the human translation.


# ------------------------------------------------------------------
# | Section (3): Adding Supplementary Paraphrase Dataset (14 points)  | 
# ------------------------------------------------------------------

# Question 3.a (/3): What is the overall BLEU score that you achieved on the test set for the part3 model?
machine_translation_with_t5_3_3_a: 
- 0.29121

# Question 3.b (/3): What is the mean BLEURT score that you achieved on the test set for the part3 model?
machine_translation_with_t5_3_3_b: 
- -0.07359

# Question 3.c (/3): What is the modern style classifier score that you got for the part3 model's generated translations?
machine_translation_with_t5_3_3_c: 
- 0.57035

# Question 3.d (/5): How do the part3 model's evaluation scores and output compare to the part2 model? You MUST answer in your notebook.
machine_translation_with_t5_3_3_d: 
- your answer



###################################################################
###################################################################
## Summarization Test (7.5 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (1): T5 for generic summarization (2.5 points)  | 
# ------------------------------------------------------------------

# Question 1.1 (/0.5): What num_beams value gives you the most readable output?
summarization_test_1_1_1: 
- 4

# Question 1.2 (/0.5): Which no_repeat_ngram_size gives the most readable output?
summarization_test_1_1_2: 
- 3

# Question 1.3 (/0.5): What min_length value gives you the most readable output?
summarization_test_1_1_3: 
- 25

# Question 1.4 (/0.5): Which max_new_tokens value gives the most readable output?
summarization_test_1_1_4: 
- 40

# Question 1.5 (/0.5): What is the ROUGE-L score associated with your most readable candidate?
summarization_test_1_1_5: 
- 0.44


# ------------------------------------------------------------------
# | Section (2): Pegasus for headline summarization (2.5 points)  | 
# ------------------------------------------------------------------

# Question 2.1 (/0.5): What num_beams value gives you the most readable output?
summarization_test_2_2_1: 
- 8

# Question 2.2 (/0.5): Which no_repeat_ngram_size gives the most readable output?
summarization_test_2_2_2: 
- 3

# Question 2.3 (/0.5): What min_length value gives you the most readable output?
summarization_test_2_2_3: 
- 20

# Question 2.4 (/0.5): Which max_new_tokens value gives the most readable output?
summarization_test_2_2_4: 
- 40

# Question 2.5 (/0.5): What is the ROUGE-L score associated with your most readable candidate?
summarization_test_2_2_5: 
- 0.76


# ------------------------------------------------------------------
# | Section (3): Pegasus for longer generation (2.5 points)  | 
# ------------------------------------------------------------------

# Question 3.1 (/0.5): What num_beams value gives you the most readable output?
summarization_test_3_3_1: 
- 8

# Question 3.2 (/0.5): Which no_repeat_ngram_size gives the most readable output?
summarization_test_3_3_2: 
- 3

# Question 3.3 (/0.5): What min_length value gives you the most readable output?
summarization_test_3_3_3: 
- 60

# Question 3.4 (/0.5): Which max_new_tokens value gives the most readable output?
summarization_test_3_3_4: 
- 100

# Question 3.5 (/0.5): What is the ROUGE-L score associated with your most readable candidate?
summarization_test_3_3_5: 
- 0.3121



###################################################################
###################################################################
## Summarization LLM Test (6.5 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (1): Llama for  summarization (6.5 points)  | 
# ------------------------------------------------------------------

# Question 1.1 (/3.5): What is the number of words in your prompt once you've met the scoring criteria?
summarization_llm_test_1_1_1: 
- 38

# Question 1.2 (/1): What is the avg ROUGE-1 score you get once you've met the scoring criteria?
summarization_llm_test_1_1_2: 
- 0.2399

# Question 1.3 (/1): What is the avg ROUGE-2 score you get once you've met the scoring criteria?
summarization_llm_test_1_1_3: 
- 0.0831

# Question 1.4 (/1): What is the avg ROUGE-L score you get once you've met the scoring criteria?
summarization_llm_test_1_1_4: 
- 0.1991
