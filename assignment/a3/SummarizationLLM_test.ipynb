{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-k1jXQHLyFU"
      },
      "source": [
        "# Assignment 3: Summarization with LLMs\n",
        "\n",
        "**Description:** This assignment covers the task of summarization which is the process of generating an abridged version of the input. With the ascendance of LLMs, we have a new way of generating summaries. Now, rather than fine-tuning. moel to generate summaries, we can simply provide explicit instructios for the summary we want the model to generate.  By finishing this assignment you should also be able to develop an intuition for:\n",
        "\n",
        "\n",
        "* How well summarization systems work\n",
        "* The effects of hyperparameters on outcomes\n",
        "* The effects of prompts on the output of an LLM\n",
        "* Evaluation of output using ROUGE\n",
        "\n",
        "\n",
        "\n",
        "This notebook must be run on a Google Colab but it does not require a GPU. By default, when you open the notebook in Colab it will NOT configure a GPU.  Summarization commands can take up to five minutes to run depending on the hyperparameters you use. This notebook will NOT run on your GCP instance as the summary models are larger than the avaialble memory.\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-summer-main/blob/master/assignment/a3/SummarizationLLM_test.ipynb)\n",
        "\n",
        "The overall assignment structure is as follows:\n",
        "\n",
        " Setup\n",
        "\n",
        "1. Gemma 2 for abstractive summarization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**:\n",
        "\n",
        "* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the **answers** file as you did in a1 and a2.\n",
        "\n",
        "* **### YOUR CODE HERE** indicates that you are supposed to write code.\n",
        "\n",
        "* In order to complete the assignment with the Gemma model you will need to get an account on [Hugging Face](https://huggingface.co).  It is free.  Once you have the account on Hugging Face you will need to create an Access Token.  Go\n",
        "to Access Token under your profile and generate a token with write permissions for colab.  You will need to copy that token and add it to the secrets in your Colab account with the name `HF_TOKEN` and the value of the string of your access token.\n",
        "\n",
        "* In addition, you will need to visit the [Model Card for the Gemma 2 model](https://huggingface.co/google/gemma-2-9b-it).  At the top of the page you will see a notice saying you need to request perrmission to use the model.  While logged in to your Hugging Face account, click the button to request permission.  It can sometimes take up to 10 or 15 minutes to get approved.  Once you are approved the message on the Model Card will change to indicate you have been granted access to the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QDMDB_KqT8m"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SYxMsJjQdK58"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U flash_attn\n",
        "!pip install -q -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA06vZYGxogD",
        "outputId": "bffefe29-b7a0-43b4-c8b3-c330c0f5cdb3"
      },
      "outputs": [],
      "source": [
        "#help track which versions of libraries we're using\n",
        "!pip list | grep transformers\n",
        "!pip list | grep accelerate\n",
        "!pip list | grep bitsandbytes\n",
        "!pip list | grep datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7_gh2dgCdL73"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from transformers import pipeline, BitsAndBytesConfig\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LQRY_zRtdL-9"
      },
      "outputs": [],
      "source": [
        "!pip install -q evaluate\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yugPWaCcdMBp"
      },
      "outputs": [],
      "source": [
        "!pip install -q rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qSkfJza6dMEI"
      },
      "outputs": [],
      "source": [
        "#let's make longer output readable without horizontal scrolling\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg8jqo1hfCdL"
      },
      "source": [
        "Now let's get the data we're going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CqV2vFZDdML9"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def load_dataset(num_samples=11):\n",
        "    \"\"\"\n",
        "    Load and sample records from the X-Sum dataset\n",
        "    \"\"\"\n",
        "    dataset = datasets.load_dataset(\"xsum\", split=\"train\", cache_dir=None, trust_remote_code=True)\n",
        "    selected_indices = random.sample(range(len(dataset)), num_samples)\n",
        "    selected_samples = dataset.select(selected_indices)\n",
        "    return selected_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIU48d57zetF",
        "outputId": "9e6e3da9-add5-464b-c44d-8ea0f160884b"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load dataset\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJJ2Fhoyz07I",
        "outputId": "dad86275-91f4-46ff-b160-59d109954148"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3qhjraguSfl"
      },
      "source": [
        "What do our input documents lok like?  Let's see the first of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "oG4L0t1BuDta",
        "outputId": "49d3f858-a62d-4e29-abb2-e59c05688437"
      },
      "outputs": [],
      "source": [
        "dataset[0]['document']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwpNL4iLuvnD"
      },
      "source": [
        "And what does the corresponding summmary look like?  This is our target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yMyEgD25uNoy",
        "outputId": "518ca8ca-8dcb-4238-df88-de6d8b902327"
      },
      "outputs": [],
      "source": [
        "dataset[0]['summary']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSQHA6KNyJHS"
      },
      "source": [
        "We'll also take advantage of a Hugging Face abstraction called a pipeline.  It is an easy way of experimenting with a model in inference mode.  We'll use that here to experiment with prompts (and possibly some hyperparameters) to imporve the quality of our results.\n",
        "\n",
        "It takes a while to load this model -- on the order of ten minutes -- but once it is loaded you can keep reusing the loaded model and improve your prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "7f5533fd460d40c8a1e76ef218972d60",
            "0599cb617df74997ab13b35a14398f14",
            "773f114ad3a744219ea313a830933bf6",
            "acacebdeabc14a5192f44f330321c518",
            "df768c2753e74968b4a1c88f2ebc337d",
            "c33b4483c49344fab1d26268d7b3ee4f",
            "093f56a50db646f99c15f21fe659d892",
            "2f599c7d25ae438099865375c6f29972",
            "9e761441afcc435495f98e591231ef11",
            "d5bd0041817e4ef6921083c1a41938c1",
            "c30deb632ad3452c9011e9cf261604c8"
          ]
        },
        "id": "Ed-EHAlY0_6h",
        "outputId": "9874ad81-80fa-49f2-9ea0-daa3ed7c57b3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initialize the pipeline with bitsandbytes quantization\n",
        "\"\"\"\n",
        "# Configure bitsandbytes for 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Initialize pipeline\n",
        "model_id = \"google/gemma-2-9b-it\"\n",
        "\n",
        "summarizer = pipeline(\n",
        "   \"text-generation\",\n",
        "   model=model_id,\n",
        "   model_kwargs={\"torch_dtype\": torch.bfloat16, \"quantization_config\": quantization_config},\n",
        "   device_map=\"auto\",\n",
        "   trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCi4E2qOon7d"
      },
      "source": [
        "As a reminder, here's the record we're dealing with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAD_slXM-LWu",
        "outputId": "88f2e4c8-4c69-4288-d1c1-41d05ac78500"
      },
      "outputs": [],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eNP71VptuPb"
      },
      "source": [
        "Let's just generate one summary so we can see what it looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y3Oa6qs4AL3A"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "            {\"role\": \"user\", \"content\": \"Generate a summary of this text: \" + dataset[0]['document']}\n",
        "        ]\n",
        "\n",
        "\n",
        "\n",
        "outputs = summarizer(\n",
        "  prompt,\n",
        "  max_new_tokens=256,\n",
        "  do_sample = True,\n",
        "  temperature = 0.3,\n",
        "  top_p = 0.95\n",
        ")\n",
        "\n",
        "summary = outputs[0][\"generated_text\"][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erKKkt4ut2Vp"
      },
      "source": [
        "Let's see what the generated summary looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2QOSE6YAzN9",
        "outputId": "e3ca3e4c-10b0-4870-9b2e-0f9a50af2534"
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjiyy3ifz76K"
      },
      "source": [
        "How does it compare with the reference? Let's compare your candidate and the reference using the ROUGE metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah7UlWgdCkQb",
        "outputId": "00e94a7e-19f7-4051-a0fd-873f5a577b9c"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "\n",
        "# Process each sample\n",
        "print(\"Generating summaries and calculating ROUGE scores...\")\n",
        "\n",
        "\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "predictions = [summary['content']]\n",
        "references = [[dataset[0]['summary']]]\n",
        "rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "rouge_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdwZYSMCsybG"
      },
      "source": [
        "Now, it's your turn.  Please improve the prompt below so that you get output that, when scored using ROUGE, the average scores for the entire data sample of 11 records exceeds these thresholds:\n",
        "* Rouge-1 > 0.2\n",
        "* Rouge-2 > 0.03\n",
        "* Rouge-L > 0.15\n",
        "\n",
        "You may use sampling with Top K or Top P and termperatire if you like but the prompt is what will have the greatest effect on your output.  Your prompt should give as specific instructions as possible.  These LLMs are trained to follow instructions so be very specific in your request.  Individual words can make a large difference so take a little time to experiment with synonyms and alternate ways of phrasing things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CNxcpDaszefb"
      },
      "outputs": [],
      "source": [
        "# Store results for aggregate scoring\n",
        "results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5wRhfLWvQK9"
      },
      "source": [
        "Enter your prompt in the space below and then run the code.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNqZgjFlXASV",
        "outputId": "a6b67ada-cdf9-4216-deea-8546635ea7b5"
      },
      "outputs": [],
      "source": [
        "dataset[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F3MKHL29hNv",
        "outputId": "4c179db4-7029-4c27-9147-518dff38bb95"
      },
      "outputs": [],
      "source": [
        "for idx, sample in enumerate(tqdm(dataset)):\n",
        "    try:\n",
        "      prompt = [\n",
        "      ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "      ### END YOUR CODE\n",
        "              ]\n",
        "\n",
        "\n",
        "      # Generate summary via the pipeline\n",
        "      outputs = summarizer(\n",
        "                          prompt,\n",
        "                          max_new_tokens=512,\n",
        "      )\n",
        "\n",
        "      summary = outputs[0][\"generated_text\"][-1]\n",
        "\n",
        "      # Calculate ROUGE scores\n",
        "      predictions = [summary['content']]\n",
        "      references = [[sample['summary']]]\n",
        "      rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
        "\n",
        "\n",
        "      # Store results\n",
        "      results.append({\n",
        "          'id': idx,\n",
        "          'original_text': sample['document'][:500],  # Store truncated text for readability\n",
        "          'reference_summary': sample['summary'],\n",
        "          'generated_summary': summary,\n",
        "           **rouge_scores\n",
        "      })\n",
        "\n",
        "      # Print progress update every 10 samples\n",
        "      if (idx + 1) % 10 == 0:\n",
        "          print(f\"\\nProcessed {idx + 1} samples\")\n",
        "          print(f\"Latest ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error processing sample {idx}: {str(e)}\")\n",
        "      continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXJOYG_Uvdq9"
      },
      "source": [
        "Calculate and print the average scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uw5sH_aLmcx",
        "outputId": "d8865fff-67cb-40e1-e22e-48286c948ffa"
      },
      "outputs": [],
      "source": [
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Calculate and print average ROUGE scores\n",
        "avg_scores = results_df[['rouge1', 'rouge2', 'rougeL']].mean()\n",
        "print(\"\\nAverage ROUGE Scores:\")\n",
        "for metric, score in avg_scores.items():\n",
        "   print(f\"{metric}: {score:.4f}\")\n",
        "\n",
        "# Print some example summaries\n",
        "print(\"\\nExample Summaries:\")\n",
        "for i in range(min(5, len(results_df))):\n",
        "   print(f\"\\nExample {i+1}:\")\n",
        "   print(f\"Reference: {results_df.iloc[i]['reference_summary']}\")\n",
        "   print(f\"Generated: {results_df.iloc[i]['generated_summary']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-cXT7xYqzN0"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.1 What is the number of words in your prompt once you've met the scoring criteria?\n",
        "\n",
        "1.2 What is the avg ROUGE-1 score you get once you've met the scoring criteria?\n",
        "\n",
        "1.3 What is the avg ROUGE-2 score you get once you've met the scoring criteria?\n",
        "\n",
        "1.4 What is the avg ROUGE-L score you get once you've met the scoring criteria?\n",
        "\n",
        "1.5 How helpful do you find ROUGE to be in creating better summaries?  How do you think it could be improved? Please write a five sentence response in the text cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFwRLMAmsIrN"
      },
      "source": [
        "*** YOUR ANSWER TO QUESTION 1.5 HERE ***\n",
        "\n",
        "*** END YOUR ANSWER ***"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
