# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 3 parts for a total of 95 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Neural Network DAN Text Classification (22 points)
# - CNN Text Classification (22 points)
# - BERT Text Classification (47 points)
# - Correct submission (2 point)
# - Answer file parses (2 point)



###################################################################
###################################################################
## Neural Network DAN Text Classification (22 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (1): Keras Functional API warm up (5 points)  | 
# ------------------------------------------------------------------

# Question 1.a (/5): I created a model using the Keras functional API that identically reproduces the model summary shown
# (This question is multiple choice.  Delete all but the correct answer).
neural_network_dan_text_classification_1_1_a: 
 - False
 - True


# ------------------------------------------------------------------
# | Section (1.1): Classification with various Word2Vec-based Models (2 points)  | 
# ------------------------------------------------------------------

# Question 1.1.a (/1): What is the percentage of positive examples in the training set (e.g. 72.575% is 0.72575)?
neural_network_dan_text_classification_1_1_1_1_a: 0.00000

# Question 1.1.b (/1): What is the percentage of positive examples in the test set (e.g. 72.575% is 0.72575)?
neural_network_dan_text_classification_1_1_1_1_b: 0.00000


# ------------------------------------------------------------------
# | Section (1.2): The Role of Shuffling of the Training Set (6 points)  | 
# ------------------------------------------------------------------

# Question 1.2.a (/3): What is the final validation accuracy that you observed after 10 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_2_1_2_a: 0.00000

# Question 1.2.b (/3): What is the final validation accuracy that you observed for the shuffled run after 10 epochs?
neural_network_dan_text_classification_1_2_1_2_b: 0.00000


# ------------------------------------------------------------------
# | Section (1.3): Approaches for Training of Embeddings (9 points)  | 
# ------------------------------------------------------------------

# Question 1.3.a (/3): What is the final validation accuracy that you observed for the static model after 10 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_3_1_3_a: 0

# Question 1.3.b (/3): What is the final validation accuracy that you observed for the model where you initialized with word2vec vectors but allow them to retrain for 3 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_3_1_3_b: 0

# Question 1.3.c (/3): What is the final validation accuracy that you observed for the model where you initialized randomly and then trained? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_3_1_3_c: 0



###################################################################
###################################################################
## CNN Text Classification (22 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (2): Stop Overfitting During Training (22 points)  | 
# ------------------------------------------------------------------

# Question 2.1.a (/2): What is the val_loss value you have after the 1st epoch of training? Copy the value in the output to the answers file. (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
cnn_text_classification_2_2_1_a: 0.00000

# Question 2.1.b (/2): What is the val_loss value you have after the 2nd epoch of training? Copy the value in the output to the answers file. (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
cnn_text_classification_2_2_1_b: 0.00000

# Question 2.1.c (/2): What is the val_loss value you have after the 3rd epoch of training? Copy the value in the output to the answers file. (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
cnn_text_classification_2_2_1_c: 0.00000

# Question 2.1.d (/2): What is the val_loss value you have after the 4th epoch of training? Copy the value in the output to the answers file. (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
cnn_text_classification_2_2_1_d: 0.00000

# Question 2.1.e (/2): What is the val_loss value you have after the 5th and final epoch of training. Copy the value in the output to the answers file. (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
cnn_text_classification_2_2_1_e: 0.00000

# Question 2.1.f (/2): What values did you use for num_filters = [] to stop the overfitting?
cnn_text_classification_2_2_1_f: your answer

# Question 2.1.g (/2): What values did you use for kernel_sizes = [] to stop the overfitting?
cnn_text_classification_2_2_1_g: your answer

# Question 2.1.h (/2): What values did you use for dense__layer_dims= [] to stop the overfitting?
cnn_text_classification_2_2_1_h: your answer

# Question 2.1.i (/2): What value did you use for dropout_rate =  to stop the overfitting?
cnn_text_classification_2_2_1_i: 0.00000

# Question 2.1.j (/2): What value did you use for embeddings_trainable = to stop the overfitting?
cnn_text_classification_2_2_1_j: False

# Question 2.1.k (/2): What values did you use for batch_size = to stop the overfitting?
cnn_text_classification_2_2_1_k: 0



###################################################################
###################################################################
## BERT Text Classification (47 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (3.1): Tokenization with BERT (15 points)  | 
# ------------------------------------------------------------------

# Question 3.1.a (/1): Why do the attention_masks have 4 and 1 zeros, respectively?
# (This question is multiple choice.  Delete all but the correct answer).
bert_text_classification_3_1_3_1_a: 
 - For the first example the last four tokens belong to a different segment. For the second one it is only the last token.
 - For the first example 4 positions are padded while for the second one it is only one.

# Question 3.1.b (/1): How many outputs are there?
bert_text_classification_3_1_3_1_b: 
- your answer

# Question 3.1.c (/1): Which output do we need to use to get token-level embeddings?
# (This question is multiple choice.  Delete all but the correct answer).
bert_text_classification_3_1_3_1_c: 
 - the first
 - the second

# Question 3.1.d (/2): Which input_id number corresponds to 'bank' in the two sentences?
bert_text_classification_3_1_3_1_d: 
- your answer

# Question 3.1.e (/2): Which token array index number corresponds to 'bank' in the first sentence?
bert_text_classification_3_1_3_1_e: 
- your answer

# Question 3.1.f (/2): Which array index number corresponds to 'bank' in the second sentence?
bert_text_classification_3_1_3_1_f: 
- your answer

# Question 3.1.g (/3): What is the cosine similarity between the BERT outputs for the two occurences of 'bank' in the two sentences?
bert_text_classification_3_1_3_1_g: 
- your answer

# Question 3.1.h (/3): How does this relate to the cosine similarity of 'this' (sentence 1) and 'the' (sentence 2). Compute the cosine similarity.
bert_text_classification_3_1_3_1_h: 
- your answer


# ------------------------------------------------------------------
# | Section (3.2): Classification with BERT (15 points)  | 
# ------------------------------------------------------------------

# Question 3.2.a (/2): What is the model checkpoint name for the most recent version of this Twitter Roberta-base sentiment analysis model?
bert_text_classification_3_2_3_2_a: your answer

# Question 3.2.b (/2): Approximately how many tweets was this latest model trained on? (You can use the abbreviation for millions like in the model card, e.g. a number like 12M or 85M.)
bert_text_classification_3_2_3_2_b: your answer

# Question 3.2.c (/2): What is the title of the published reference paper for this most recent model?
bert_text_classification_3_2_3_2_c: your answer

# Question 3.2.d (/5): What is the final validation accuracy that you observed for the Twitter RoBERTa sentiment-trained model after training for 2 epochs?
bert_text_classification_3_2_3_2_d: 
- your answer

# Question 3.2.e (/2): Did the Twitter RoBERTa sentiment-trained model do better or worse or the same as the BERT-base?
# (This question is multiple choice.  Delete all but the correct answer).
bert_text_classification_3_2_3_2_e: 
 - RoBERTa better
 - RoBERTa worse
 - Same

# Question 3.2.f (/2): Answer in the notebook
bert_text_classification_3_2_3_2_f: your answer


# ------------------------------------------------------------------
# | Section (3.2): Freezing BERT components (17 points)  | 
# ------------------------------------------------------------------

# Question 3.3.a (/5): What is the final validation accuracy that you observed for this lowest level unfrozen version of the BERT classification model after training for 2 epochs?  (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_a: 0.00000

# Question 3.3.b (/1): What is the final training loss that you observed for your better performing version of the BERT classification model after training for 2 epochs?  (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_b: 0.00000

# Question 3.3.c (/1): What is the final validation loss that you observed for your better performing version of the BERT classification model after training for 2 epochs?  (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_c: 0.00000

# Question 3.3.d (/2): What is the ratio of your final training loss/final validation loss? For this better version the ratio must be greater than 0.7.  (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_d: 0.00000

# Question 3.3.e (/2): What is the final validation accuracy that you observed for your better performing version of the BERT classification model after training for 2 epochs??  (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_e: 0.00000

# Question 3.3.f (/1): What is the final training loss that you observed for your overfitting version of the BERT classification model after training for 2 epochs?  (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_f: 0.00000

# Question 3.3.g (/1): What is the final validation loss that you observed for your overfitting version of the BERT classification model after training for 2 epochs?  (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_g: 0.00000

# Question 3.3.h (/2): What is the ratio of your final training loss/final validation loss? For this overfitting version, the ratio must be less than 0.5.  (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_h: 0.00000

# Question 3.3.i (/2): What is the final validation accuracy that you observed for your overfitting version of the BERT classification model after training for 2 epochs??  (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_i: 0.00000
