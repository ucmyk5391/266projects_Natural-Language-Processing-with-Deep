{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELZzyfJxvt1X"
      },
      "source": [
        "# Assignment 3: Machine Translation with T5\n",
        "\n",
        "**Description:** This assignment notebook builds on the material from the\n",
        "[lesson 6 notebook](https://github.com/datasci-w266/2025-summer-main/blob/master/materials/lesson_notebooks/lesson_6_Machine_Translation_With_Transformer.ipynb), in which we set up a new, very small version of a T5 encoder decoder model to train from scratch on translations from Shakespearean to Modern English. Since the model was trained from scratch, it didn't work very well. In this notebook, we'll first try to make that model work a little better, changing the model configuration and output generation parameters. Then we'll fine tune a small pre-trained T5 model on this task, to see how much better we can do with even a small pre-trained model. We'll apply several evaluation metrics, find some trade-offs, and try adding a secondary dataset to address some of the remaining challenges.\n",
        "\n",
        "This notebook should be run on a Google Colab leveraging a GPU. By default, when you open the notebook in Colab it will try to use a GPU. Since colab is providing free access to a GPU they place constraints on that access.  Therefore you might want to turn off the GPU access (Edit -> Notebook Settings) while editing and initially debugging your code (at least the setup before you train each model). You will need a GPU to full train or evaluate each of the models. Total runtime of the entire notebook (with solutions and a Colab GPU) should be about 1-2h, but potentially more depending on how much you experiment. If Colab tells you that you have reached your GPU limit, wait 10-24 hours and you should be able to access a GPU again.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2024-fall-main/blob/master/assignment/a3/Machine_Translation_T5.ipynb)\n",
        "\n",
        "The overall assignment structure is as follows:\n",
        "\n",
        "\n",
        "0. Setup\n",
        "  \n",
        "  0.1 Libraries\n",
        "\n",
        "  0.2 Data Acquisition\n",
        "\n",
        "  0.3. Data Preparation\n",
        "\n",
        "\n",
        "1. Tiny Seq2Seq Model Trained From Scratch\n",
        "  \n",
        "  1.1 Tokenizer and Model Setup\n",
        "\n",
        "  1.2 Experimenting with Model Dimensions\n",
        "\n",
        "  1.3 Text Generation Parameters\n",
        "\n",
        "  1.4 Test Set Evaluation Metrics\n",
        "\n",
        "2. Small Pre-Trained T5 Model\n",
        "\n",
        "  2.1 Pre-Trained Model Setup and Tokenization\n",
        "\n",
        "  2.2 Fine-Tuning the Pre-Trained Model\n",
        "\n",
        "  2.3 Fine-Tuned Model Evaluation\n",
        "\n",
        "  2.4 Style Classifier\n",
        "\n",
        "  2.5 Revisit Decoder .Generate() Options\n",
        "\n",
        "3. Adding Supplementary Paraphrase Dataset\n",
        "\n",
        "  3.1 Load and preprocess the supplemental dataset\n",
        "\n",
        "  3.2 Train T5 on Paraphrasing Task\n",
        "\n",
        "  3.3 Fine-Tune Paraphrase-Trained Model on Main Task\n",
        "  \n",
        "  3.4 Paraphrase-Trained Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUtX9PLp2Ytz"
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "### 0.1 Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIz3zJzLvuCp",
        "outputId": "4f8762e2-b58c-45cd-9e45-5880b1b8a39b"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U evaluate\n",
        "!pip install -q -U tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b0sbeNQSLnjt"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import evaluate\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# For from-scratch T5 model\n",
        "from transformers import T5TokenizerFast, T5Config, T5ForConditionalGeneration\n",
        "\n",
        "# For pre-trained T5 model\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration  # this won't import twice, just noting here what's for each model\n",
        "\n",
        "# For all T5 models\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# For BLEURT (to load a trained model for evaluation)\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# For style classifier model (also for evaluating the seq2seq model output)\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJM6ndNBPaSk"
      },
      "source": [
        "### O.2 Data Acquisition\n",
        "\n",
        "We'll use the Shakespeare-to-Modern-English translation dataset from Lesson 6. The data includes aligned sentences from a number of plays by William Shakespeare.\n",
        "\n",
        "The data was copied from this repo --[https://github.com/cocoxu/Shakespeare](https://github.com/cocoxu/Shakespeare) -- and consolidated into one file for easier handling.\n",
        "\n",
        "You will to grab a copy from our git repo and import it to your Google drive.  From there you'll be able to easily load it in to a Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8rtGshYA3PG",
        "outputId": "8c7e4199-72b0-45ce-fddb-cdb55b0cd670"
      },
      "outputs": [],
      "source": [
        "# This cell will authenticate you and mount your Drive in the Colab.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K7L5o9M3A3SN"
      },
      "outputs": [],
      "source": [
        "# Modify this path to the appropriate location in your Drive\n",
        "text_file = 'drive/MyDrive/ISchool/MIDS/266/data/train_plays-org-mod.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h92xVgt2nRo"
      },
      "source": [
        "### O.3 Data Preparation\n",
        "\n",
        "Each line contains a Shakespearean sentence and its corresponding modern English translation.\n",
        "\n",
        "The Shakesperean sentence is the *source sequence* and modern English one is the *target sequence*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0uUVC8dEA3VY"
      },
      "outputs": [],
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    old, mod = line.split(\"\\t\")\n",
        "    old = old\n",
        "    mod = mod\n",
        "    text_pairs.append((old, mod))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94IzHT9OPk92",
        "outputId": "28b1f2aa-9a65-4bf4-bd14-8d2d355bc8b4"
      },
      "outputs": [],
      "source": [
        "# Look at some examples\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf-BizHPPlUn",
        "outputId": "55343b69-c3e7-40f8-d3b8-2e1a9f422931"
      },
      "outputs": [],
      "source": [
        "# Let's create some splits\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.06 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLrZzsXW_X7W"
      },
      "source": [
        "Like we did in the lesson notebook, let's create a Huggingface dataset object from our data, so that it's easy to work with and pass to our model trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gilXYB0oQgDM"
      },
      "outputs": [],
      "source": [
        "def make_dataset(pairs):\n",
        "    org_texts, mod_texts = zip(*pairs)\n",
        "    org_texts = list(org_texts)\n",
        "    mod_texts = list(mod_texts)\n",
        "\n",
        "    dataset = Dataset.from_dict({\"shakespeare\": org_texts, \"modern\": mod_texts})\n",
        "    return dataset.shuffle()\n",
        "\n",
        "# Make the training data\n",
        "train_dataset = make_dataset(train_pairs)\n",
        "\n",
        "# Make the validation data\n",
        "val_dataset = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyOtMY5wPq6J"
      },
      "source": [
        "## 1. Tiny Seq2Seq Model Trained From Scratch\n",
        "\n",
        "As in the lesson 6 notebook, for our first model, we'll make a new tokenizer and model based on the T5 architecture, which we'll train from scratch only on our task dataset.\n",
        "\n",
        "### 1.1 Tokenizer and Model Setup\n",
        "\n",
        "The easiest way to make a new tokenizer is to load an existing T5 one, then call .train_new_from_iterator(), providing our own dataset and vocab size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7lBXb7hVPoJ1"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 15000\n",
        "\n",
        "def get_word_piece_tokenizer(text_samples, vocab_size):\n",
        "\n",
        "    base_tokenizer = T5TokenizerFast.from_pretrained('t5-base')\n",
        "    new_tokenizer = base_tokenizer.train_new_from_iterator(\n",
        "        text_samples,\n",
        "        vocab_size=VOCAB_SIZE\n",
        "    )\n",
        "\n",
        "    return new_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-rmXZ8AWQHzI"
      },
      "outputs": [],
      "source": [
        "shakespeare_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "modern_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "\n",
        "part1_tokenizer = get_word_piece_tokenizer(shakespeare_samples + modern_samples, VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqREBXE_DQV"
      },
      "source": [
        "We'll need to preprocess the data using the tokenizer. Since our task is to translate from Shakespearean to Modern English, the Shakespeare text will be our input_ids and the Modern English will be the labels we use for training and evaluation. We'll create a function to do the tokenization, and then map it to our Huggingface datasets containing the train and validation data. We'll have the function take a tokenizer, because later we'll use a different pre-trained one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xyypul0YQH1s"
      },
      "outputs": [],
      "source": [
        "MAX_SEQUENCE_LENGTH = 40\n",
        "\n",
        "def preprocess_translation_batch(batch_text_pairs, tokenizer, prefix=\"\"):\n",
        "    if prefix:\n",
        "        batch_text_pairs[\"shakespeare\"] = [prefix + text for text in batch_text_pairs[\"shakespeare\"]]\n",
        "\n",
        "    shakespeare_encoded = tokenizer.batch_encode_plus(\n",
        "        batch_text_pairs[\"shakespeare\"],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    modern_encoded = tokenizer.batch_encode_plus(\n",
        "        batch_text_pairs[\"modern\"],\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    return {'input_ids': shakespeare_encoded['input_ids'],\n",
        "            'labels': modern_encoded['input_ids']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "ca7cc02119e04c5bb02871d174ba455c",
            "2886837fbd044e1e8c65d474994ef6a4",
            "5db15f88991e44e6869ce55a7f21e5ac",
            "c88fdd938632483a9eaa65db12d97019",
            "1e866beb6b294367a144ae3e0b71c7fb",
            "53ac97ac60ec4b2b9b269711e7a85dbe",
            "d31c95af4e944209a8bcbc0b620b0c06",
            "57a0bde2bba14222abf7bd4a0a95c196",
            "bd06468ac387454ba4a5aa54d116eeec",
            "8c145d497ea44e04845361cce9df3fc5",
            "a66b9616c2044d0bb5989f2e38d7a07e",
            "1a53354e87b34f3585a89b0f95d00a37",
            "2e03cc09195e40a88c11d85b4d1007ce",
            "f22271f3810c4204921db7a07a12754f",
            "a969bdc5cac0412dba6392ba42dbf606",
            "3b7a293fbfb040e382d8b5de76e3728e",
            "100fe38552b142288a48827cec877e4f",
            "3b337389dc0042b09d7c06e275d8d862",
            "56253b18fc264084a190ba569267e660",
            "63fce312f7b44259b56413ea2cb63797",
            "ec856a31c15548a499c07f9374750026",
            "67aee4c264a54aa5b8a078d79a271274"
          ]
        },
        "id": "PP4hp7d9QH4V",
        "outputId": "7c28c1e3-25bb-416c-82e1-c986198890b9"
      },
      "outputs": [],
      "source": [
        "train_ds_part1 = train_dataset.map(preprocess_translation_batch, batched=True,\n",
        "                                   fn_kwargs={'tokenizer': part1_tokenizer})\n",
        "val_ds_part1 = val_dataset.map(preprocess_translation_batch, batched=True,\n",
        "                               fn_kwargs={'tokenizer': part1_tokenizer})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrqQr1Zby0O3"
      },
      "source": [
        "We'll need to create the new model from a config, specifying the model's dimensions. Then we'll need to make training arguments and trainer objects to be able to train the model. Let's create a function for each of those purposes, so that later we can use the functions to experiment with the available options.\n",
        "\n",
        "First, make a function to create the model config and the model itself. Use the Lesson 6 notebook as a guide, and make sure to include all of the arguments that we've included in the function definition below. Those are what you'll experiment with next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uLvnNTVGy0cw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fill in the code to create a T5Config and new T5 model, using all of the function arguments\n",
        "\"\"\"\n",
        "\n",
        "def create_from_scratch_model(num_layers, embed_dim, keyvalue_dim, dense_dim, num_heads):\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # t5_config = ...\n",
        "    # t5_model = ...\n",
        "\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return t5_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSdOLcYszpda"
      },
      "source": [
        "We'll also need to specify training arguments and a trainer for our model. Use the Seq2SeqTrainingArguments and Seq2SeqTrainer classes imported at the top of this notebook. You can use the Lesson 6 notebook as a guide for this too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "h6kk5VtzzpoX"
      },
      "outputs": [],
      "source": [
        "def create_seq2seq_training_args(batch_size, num_epochs):\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # training_args = ...\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "T-tiQ4GIzpuQ"
      },
      "outputs": [],
      "source": [
        "def create_seq2seq_trainer(model, training_args, train_ds, val_ds):\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # trainer = ...\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITxgfivHQpuP"
      },
      "source": [
        "### 1.2: Experimenting with Model Dimensions\n",
        "\n",
        "In the Lesson 6 Notebook, we created a very small T5-style model with just one transformer layer and smaller dimensions for some of the internal layers. Now, you'll explore these options yourself, to see if you can get the model to work a little better when trained on this task.\n",
        "\n",
        "Without adding any additional training data, can we configure the model to perform better when trained on this task? What happens if we add another one or more transformer layers to the encoder and decoder, or make some of the internal dimensions smaller or larger?\n",
        "\n",
        "The T5Config gives us several hyperparameters to adjust the model's parameter dimensions. You can see the available arguments and their default values in the [T5Config documentation](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/t5#transformers.T5Config).\n",
        "\n",
        "We'll give you the batch size and num_epochs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "B6AgagA7XkMP"
      },
      "outputs": [],
      "source": [
        "part1_batch_size = 64\n",
        "part1_num_epochs = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qOChzNoTWOd"
      },
      "source": [
        "Now you decide the rest.\n",
        "\n",
        "Try changing the values for *num_layers* (number of transformer blocks), *d_model* (size of embedding and pooler layers), *d_kv* (size of query, key, and value vectors per attention head), *num_heads* (the number of attention heads), and *d_ff* (size of feed forward layers after each attention layer).\n",
        "\n",
        "Find hyperparameters that finish training 30 epochs in 10-20 minutes on a free Colab T4 GPU, and that give you as low of a validation loss as you can, at least below 1.8. Also try to do this without overwhelming overfitting, i.e. try to keep training_loss / validation_loss > 0.6 after 30 epochs.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "babjqnZQIPcA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Define the values you want to use for d_model, d_kv, num_heads, and d_ff, for the T5Config below.\n",
        "\"\"\"\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# embed_dim = ...\n",
        "# keyvalue_dim = ...\n",
        "# num_heads = ...\n",
        "# dense_dim = ...\n",
        "# num_layers = ...\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "54FG64BEIXDb",
        "outputId": "87ea181a-d6ee-40e6-ec45-788383c7c5b5"
      },
      "outputs": [],
      "source": [
        "part1_model = create_from_scratch_model(num_layers, embed_dim, keyvalue_dim, dense_dim, num_heads)\n",
        "part1_training_args = create_seq2seq_training_args(part1_batch_size, part1_num_epochs)\n",
        "part1_trainer = create_seq2seq_trainer(part1_model, part1_training_args,\n",
        "                                       train_ds_part1, val_ds_part1)\n",
        "\n",
        "part1_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cwO4nWICZa6"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 1.a What is the final validation loss that you were able to achieve for the part1 model after training for 30 epochs? (Copy and paste the decimal value for the final validation loss, to 5 significant digits, e.g. a number like 0.56781 or 0.87632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 1.b Which model config parameters (if any) did you increase, to achieve a lower validation loss, while staying within the training time and overfitting guidelines? (List the names of the parameters you increased, e.g. embed_dim, keyvalue_dim, num_heads, dense_dim, num_layers. Put this list in square brackets in the answers file.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 1.c Which model config parameters (if any) did you decrease, to achieve a lower validation loss, while staying within the training time and overfitting guidelines? (List the names of the parameters you decreased, e.g. embed_dim, keyvalue_dim, num_heads, dense_dim, num_layers. Put this list in square brackets in the answers file.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Tdtz6k6ZwG4G"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Before moving on, save a checkpoint of the model you just trained in your Drive,\n",
        "So that you can pick up where you left off later if needed\n",
        "\"\"\"\n",
        "\n",
        "# Modify this path to the location in your Drive where you want to save the part1 model\n",
        "part1_model_checkpoint_filepath = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/part1_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ucobq-zywIYQ"
      },
      "outputs": [],
      "source": [
        "# Run this line only after you've trained the part1 model\n",
        "part1_model.save_pretrained(part1_model_checkpoint_filepath, from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cou4qT7-wIah"
      },
      "outputs": [],
      "source": [
        "# Run this line only if you need to reload the model you trained earlier\n",
        "part1_model = T5ForConditionalGeneration.from_pretrained(part1_model_checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us-I-FEMAkue"
      },
      "source": [
        "### 1.3: Text Generation Parameters\n",
        "\n",
        "Cross-entropy loss is great for training, but it's not a very interpretable metric for manually reviewing how well the model is doing as we experiment with available options. Ultimately, we want to actually look at the translations the model outputs, compare them to human translations, and potentially judge other aspects of the actual output.\n",
        "\n",
        "To do that, we need to actually generate some model output. Remember that the model itself predicts probabilities for each word in the vocabulary, based on what words have already been generated, at each decoder time-step. In order to select which actual words to output, there are multiple decoder strategies we can use that are build on top of the model's predicted probabilities. (E.g. beam search, top-k or top-p sampling, repeat ngram constraints, min/max length constraints, etc.)\n",
        "\n",
        "Let's define a function below to generate translations for new inputs. Then we'll define another function to translate the validation set and calculate some standard evaluation metrics for translation, as well as print out some translations for manual inspection. We'll include some arguments that you'll experiment with next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mL_ldFqfAiH5"
      },
      "outputs": [],
      "source": [
        "def generate_output(model, tokenizer, input_sentences, batch_size, **kwargs):\n",
        "\n",
        "    all_outputs = []\n",
        "\n",
        "    for i in range(int(len(input_sentences) / batch_size) + 1):\n",
        "        start_i, end_i = i * batch_size, (i + 1) * batch_size\n",
        "        if start_i >= len(input_sentences):\n",
        "            break\n",
        "\n",
        "        inputs_encoded = tokenizer(input_sentences[start_i:end_i], padding=True, return_tensors='pt')\n",
        "        output_ids = model.cuda().generate(inputs_encoded['input_ids'].cuda(), **kwargs)\n",
        "        generated_sentences = tokenizer.batch_decode(output_ids,\n",
        "                                                     skip_special_tokens=True,\n",
        "                                                     clean_up_tokenization_spaces=False)\n",
        "        all_outputs.extend(generated_sentences)\n",
        "\n",
        "    return all_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LGd3bA67rEX2"
      },
      "outputs": [],
      "source": [
        "# Load the BLEU metric and the trained BLEURT model for semantic similarity scoring\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "bleurt_tokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-base-512\")\n",
        "bleurt_model = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-base-512\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "I3WkHhO_2KQJ"
      },
      "outputs": [],
      "source": [
        "def calculate_eval_metrics(text_pairs, model, tokenizer, batch_size, prefix=\"\", **kwargs):\n",
        "    original_texts = [prefix + pair[0] for pair in text_pairs]\n",
        "    label_texts = [pair[1] for pair in text_pairs]\n",
        "\n",
        "    # Translate original texts\n",
        "    translations = generate_output(model, tokenizer, original_texts, batch_size, **kwargs)\n",
        "\n",
        "    # Calculate BLEU scores\n",
        "    bleu_results = bleu.compute(predictions=translations, references=label_texts)\n",
        "    print('BLEU: ', bleu_results)\n",
        "\n",
        "    # Calculate BLEURT scores\n",
        "    bleurt_scores = []\n",
        "    for i in range(int(len(translations) / batch_size) + 1):\n",
        "        start_i, end_i = i * batch_size, (i + 1) * batch_size\n",
        "        if start_i >= len(translations):\n",
        "            break\n",
        "\n",
        "        with torch.no_grad():\n",
        "            scores = bleurt_model(**bleurt_tokenizer(label_texts[start_i:end_i],\n",
        "                                                     translations[start_i:end_i],\n",
        "                                                     truncation=True,\n",
        "                                                     max_length=MAX_SEQUENCE_LENGTH,\n",
        "                                                     padding='max_length',\n",
        "                                                     return_tensors='pt'))[0].squeeze().numpy()\n",
        "            if scores.shape:\n",
        "                bleurt_scores.extend(scores)\n",
        "            else:  # Happens when there was only one example in the last batch\n",
        "                bleurt_scores.append(float(scores))\n",
        "\n",
        "    print('BLEURT: ', np.mean(bleurt_scores))\n",
        "\n",
        "    return translations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_s2S5nzHS2R"
      },
      "source": [
        "First, choose some keyword arguments to pass to the generate_output() function. These can be any parameters for the .generate() method (e.g. beam search or top-k or top-p sampling, no_repeat_ngram_size, etc). You will want to try the options listed in Question 1.e below, to be able to answer that question (but some of them can't be used at the same time). More info on each can be found in the [Huggingface documentation on text generation here](https://huggingface.co/docs/transformers/en/main_classes/text_generation).\n",
        "\n",
        "Then run the function to translate the validation set and print out eval metrics. The function returns the translations, so we'll also print out a sample of those to manually inspect. Use what you see to iterate on the .generate() arguments, trying to find the most reasonable .generate() arguments that you can for the model you trained.\n",
        "\n",
        "The output will not be great no matter what you do, but you should be able to make it a little more readable, with slightly better BLEU and BLEURT metrics, than the basic options specified in the Lesson 6 notebook.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFBbfA4I4Anr",
        "outputId": "b98697f7-2459-4ee5-ca87-46bd1bbe44ea"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fill in the decoder .generate() arguments that you want to use, like num_beams or top_p, etc.\n",
        "\"\"\"\n",
        "\n",
        "part1_generate_kwargs = {\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "}\n",
        "\n",
        "part1_val_translations = calculate_eval_metrics(\n",
        "    val_pairs,\n",
        "    part1_model,\n",
        "    part1_tokenizer,\n",
        "    part1_batch_size,\n",
        "    **part1_generate_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFwH8PNhdzvC",
        "outputId": "a772ef70-3fc1-49a2-a758-0c675a1ef157"
      },
      "outputs": [],
      "source": [
        "# Print out a sample of outputs to manually review\n",
        "for i in range(10):\n",
        "    sample_i = random.choice(range(len(part1_val_translations)))\n",
        "    print('Original:    ', val_pairs[sample_i][0])\n",
        "    print('Reference:   ', val_pairs[sample_i][1])\n",
        "    print('Translation: ', part1_val_translations[sample_i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJtxjFFYeUIq"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 1.d What seems to be particularly bad about the part1 model's translations? (Choose one of the following options that you agree with most and put it in the answers file.)\n",
        "\n",
        " - A. The model keeps repeating the same common words or phrases over and over, which don't produce very meaningful statements.\n",
        "\n",
        " - B. The model is generating pretty good modern English, but it's quite offensive.\n",
        "\n",
        " - C. The model's output has mostly the same meaning as the input, but with minor grammatical mistakes.\n",
        "\n",
        " - D. The model is making up elaborate narrative details that don't appear in the original text.\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 1.e Which .generate() parameter seemed to help the most in addressing the main shortcoming(s) that you noticed in the part1 model's output? (Choose one of the following options and put it in the answers file.)\n",
        "\n",
        " - A. num_beams\n",
        " - B. do_sample\n",
        " - C. top_k\n",
        " - D. top_p\n",
        " - E. temperature\n",
        " - F. no_repeat_ngram_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS7KfyCVeK-O"
      },
      "source": [
        "### 1.4 Test Set Evaluation Metrics\n",
        "\n",
        "Once you've settled on training hyperparameters that produce good validation loss, and generation options that produce the best output you can so far, go ahead and calculate evaluation metrics on the test set, to warp up this from-scratch model.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-yWnDm_Cphf",
        "outputId": "41062ddf-c72d-4708-8e05-b5f4708c627d"
      },
      "outputs": [],
      "source": [
        "# Print out eval metrics for the part1_model on the test set\n",
        "\n",
        "part1_test_translations = calculate_eval_metrics(\n",
        "    test_pairs,\n",
        "    part1_model,\n",
        "    part1_tokenizer,\n",
        "    part1_batch_size,\n",
        "    **part1_generate_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn8KX1eOAiRa"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 1.f What is the overall BLEU score that you achieved on the test set for the part1 model? (Copy and paste the decimal value for the overall BLEU score, to 5 significant digits, e.g. a number like 0.03671 or 0.09763. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 1.g What is the mean BLEURT score that you achieved on the test set for the part1 model? (Copy and paste the decimal value for the mean BLEURT score, to 5 significant digits, e.g. a number like -1.12345 or -0.54321. Put the answer in the answers file; it should match the value shown in your output in this notebook.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doyqgjOWzPy4"
      },
      "source": [
        "## 2. Small Pre-Trained T5 Model\n",
        "\n",
        "What if we use a model that has already been pre-trained to recognize English (at least modern English), even if it hasn't yet been trained for our particular translation task?\n",
        "\n",
        "We'll use a T5 small model, which should be able to generate good modern English, but we'll need to train it to encode and translate Shakespearean text.\n",
        "\n",
        "### 2.1 Pre-trained Model Setup and Tokenization\n",
        "\n",
        "The next two cells load the pre-trained model, and preprocess the data with the pre-trained tokenizer. Fill in the necessary code for each of these cells.\n",
        "\n",
        "For preprocessing, you'll need to map the `preprocess_translation_batch` function that we created earlier to the `train_dataset` and `val_dataset`. Use the code from part 1 as an example, but now pass in the pretrained T5 tokenizer as a function keyward argument (kwarg). Also pass in the given task_prefix as the \"prefix\" kwarg for the preprocessing function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GcSwKbx5s9k",
        "outputId": "eacdb2e1-2888-4f1b-85ce-8396b603a5a8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Load the pre-trained model and tokenizer\n",
        "\"\"\"\n",
        "\n",
        "t5_pretrained_checkpoint_name = 'google-t5/t5-small'\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# part2_tokenizer = ...\n",
        "# part2_model = ...\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "1ea6e81e6a0c45f991a0b8588493a15d",
            "ec4a9770fe094892865bcf67ab5004cf",
            "3b0d6659521044019e6bec3eef33e484",
            "5065b87f77d44cc88413933dee621c64",
            "495258fac90d4b358b91c5618ad1d103",
            "7eeadc4f4cbf4e67b87ffca6cb1848d6",
            "3a53214b00864c878e69d637575ca9c4",
            "77e210f75cd14c7db3bbb7c873d6f6b8",
            "29b1dfca50fa4c0f989af38cdf875a15",
            "1bbb267bca0741d1af4ef0d26258f29f",
            "37b0ff049d7b4412b6519b33bc0e05e8",
            "c7682948eeda447c8f864385bac442ad",
            "5f4e67077e3f4d96af3c074b7273a660",
            "5087203f2d874b22887f48199f4d7a57",
            "03db34cb06fb4bd387bfec7c0d70dca0",
            "9dccbeb9df994974a9c55ca668d06a02",
            "fb2aeb9f7da84b3da3bfd83a6b1f44cd",
            "b4c3827aab6241488ba30846e170c455",
            "491e9a25224f4a1c91812c8d60600d01",
            "c662c9a3dfb343bc92b6c471e0c44e05",
            "d8ff10ca7fd746459f688e57542482e6",
            "c6abe6df07524df2a9ee3975be45e69b"
          ]
        },
        "id": "e3W6E_j3-tV8",
        "outputId": "48e2f4c8-2944-45f3-b1cf-9cdd86b4f63d"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Preprocess the datasets using the pretrained tokenizer, and the given task_prefix.\n",
        "Use the task_prefix as the \"prefix\" argument to the function preprocess_translation_batch().\n",
        "\"\"\"\n",
        "\n",
        "task_prefix = 'Translate Shakespeare to Modern English: '\n",
        "\n",
        "train_ds_part2 = train_dataset.map(\n",
        "    preprocess_translation_batch,\n",
        "    batched=True,\n",
        "    fn_kwargs={\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "})\n",
        "\n",
        "val_ds_part2 = val_dataset.map(preprocess_translation_batch,\n",
        "    batched=True,\n",
        "    fn_kwargs={\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP4FNVdVTO3V"
      },
      "source": [
        "### 2.2 Fine-Tuning the Pre-Trained Model\n",
        "\n",
        "Now create the training args and trainer to fine-tune this pre-trained model. We've given you part of the code: you'll use the same functions as above for `create_seq2seq_training_args` and `create_seq2seq_trainer`. Fill in the rest of the arguments that you need for this version of the model. Use the provided batch size and num_epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7LyTR1ha-tYy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create the training args and trainer for the pre-trained model.\n",
        "Use the batch size and num_epochs provided below for this model.\n",
        "\"\"\"\n",
        "\n",
        "part2_batch_size = 32\n",
        "part2_num_epochs = 4\n",
        "\n",
        "part2_training_args = create_seq2seq_training_args(\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        ")\n",
        "\n",
        "part2_trainer = create_seq2seq_trainer(\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u14PQfNAjdV"
      },
      "source": [
        "Run the cell below to fine-tune the part2 model, then answer the following questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "jvffobrggOAG",
        "outputId": "4d6b2989-dee1-40ae-b569-a7c42351e5ec"
      },
      "outputs": [],
      "source": [
        "part2_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2TswETsAoQj"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 2.a What is the final validation loss that you were able to achieve for the part2 model after training for 4 epochs? (Copy and paste the decimal value for the final validation loss, to 5 significant digits, e.g. a number like 0.56781 or 0.87632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GtDbBBE4w_gx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Before moving on, save a checkpoint of the model you just trained in your Drive,\n",
        "So that you can pick up where you left off later if needed\n",
        "\"\"\"\n",
        "\n",
        "# Modify this path to the location in your Drive where you want to save the part2 model\n",
        "part2_model_checkpoint_filepath = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/part2_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ouQMAZoww_t1"
      },
      "outputs": [],
      "source": [
        "# Run this line only after you've fine-tuned the part2_model\n",
        "part2_model.save_pretrained(part2_model_checkpoint_filepath, from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NGN1544jw_yZ"
      },
      "outputs": [],
      "source": [
        "# Run this line only if you need to reload the model you fine-tuned earlier\n",
        "part2_model = T5ForConditionalGeneration.from_pretrained(part2_model_checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyv34a6Igjbf"
      },
      "source": [
        "### 2.3 Fine-Tuned Model Evaluation\n",
        "\n",
        "Now use the calculate_eval_metrics() function defined above to translate the test set and calculate evaluation metrics. Also print out a sample of the translated outputs. For now, use the same decoder .generate() kwargs that you chose for part1.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C00JmnS_EEr",
        "outputId": "62f4addc-44fa-445a-972e-8cc28de6a2ce"
      },
      "outputs": [],
      "source": [
        "# Print out eval metrics for the part2_model on the test set\n",
        "\n",
        "part2_test_translations = calculate_eval_metrics(\n",
        "    test_pairs,\n",
        "    part2_model,\n",
        "    part2_tokenizer,\n",
        "    part2_batch_size,\n",
        "    task_prefix,\n",
        "    **part1_generate_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n47QLl_KshfG",
        "outputId": "e25a6ca2-bed3-440c-f567-4365c6551f35"
      },
      "outputs": [],
      "source": [
        "# Print out a sample of the translated outputs to look at as well\n",
        "\n",
        "for i in range(10):\n",
        "    sample_i = random.choice(range(len(part2_test_translations)))\n",
        "    print('Original:    ', test_pairs[sample_i][0])\n",
        "    print('Reference:   ', test_pairs[sample_i][1])\n",
        "    print('Translation: ', part2_test_translations[sample_i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NZ8u1NaAFcf"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 2.b What is the overall BLEU score that you achieved on the test set for the part2 model? (Copy and paste the decimal value for the overall BLEU score, to 5 significant digits, e.g. a number like 0.03671 or 0.09763. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.c What is the mean BLEURT score that you achieved on the test set for the part2 model? (Copy and paste the decimal value for the mean BLEURT score, to 5 significant digits, e.g. a number like -1.12345 or -0.54321. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.d What do you notice about the part2 model's output? It should be much better than the part1 model's output. But the translations still probably don't perfectly match the reference human translations. What does the part2 model seem to still be doing poorly? (Chose one of the following options that you agree with most, and put it in the answers file.)\n",
        "\n",
        " - A. The generated translations are gibberish.\n",
        "\n",
        " - B. The generated translations are written in a far more casual style than the reference human translations.\n",
        "\n",
        " - C. The generated translations mean something completely different from the input text and reference translations.\n",
        "\n",
        " - D. The generated translations are too similar to the input text, and haven't been rephrased as much as the reference human translations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfJCnESMdAR5"
      },
      "source": [
        "### 2.4 Style Classifier\n",
        "\n",
        "Now that the model is able to output more coherent translations, we can start to get more picky about different aspects of the output. We should also make sure that our evaluation metrics are capturing everything we want to be able to assess and improve in the model's output.\n",
        "\n",
        "One thing we're not capturing yet is if the output has the right **style**. This task is sort of a translation task, but since it's between two different forms of English, we can also think of it as a style transfer task.\n",
        "\n",
        "BLEU might help a little with that, but when the model chooses different words from the human reference, it could do so in ways that are still good modern English or that are still too much like Shakespeare. BLEURT won't tell us anything about the style, as long as the meaning is still similar to the reference.\n",
        "\n",
        "How can we tell whether the output has the right style? We could train a separate classification model to predict whether text is Shakespearean or modern English. We have the data to do it! We just need to repurpose our data for a classification problem.\n",
        "\n",
        "Use the code below to train a BERT classifier to predict whether a sentence is Shakespearean or modern English. We're providing this code for you, because it's not the main task and not based on a similar example from class. We want you to use it as one of your evaluation metrics, to help you iterate on your models for the main task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "xDQEZITtdAe6"
      },
      "outputs": [],
      "source": [
        "def make_style_classifier_data(text_pairs):\n",
        "    style_texts = [pair[0] for pair in train_pairs] + [pair[1] for pair in train_pairs]\n",
        "    style_labels = [0 for pair in train_pairs] + [1 for pair in train_pairs]\n",
        "\n",
        "    style_dataset = Dataset.from_dict({\"text\": style_texts, \"label\": style_labels})\n",
        "\n",
        "    return style_dataset.shuffle()\n",
        "\n",
        "style_train_ds = make_style_classifier_data(train_pairs)\n",
        "style_valid_ds = make_style_classifier_data(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpKXPqTZpj_a",
        "outputId": "8ea016ba-4e9f-4188-9b8d-c55cb1bece62"
      },
      "outputs": [],
      "source": [
        "bert_checkpoint_name = 'bert-base-cased'\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(bert_checkpoint_name)\n",
        "bert_style_classifier_model = BertForSequenceClassification.from_pretrained(bert_checkpoint_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "eb73b8e3e77541c5bd8f0254b29ecaa1",
            "3b914ab1e3ef41c68a99621b8e5d5920",
            "fa371359dbdc4ab28a50b0d36b52d050",
            "ca8eae65d7734095a59add8598c7fdd0",
            "81d1a7869796410eacbb4cbeccd1a925",
            "8842f373e0c1425aa62a0c971f42783d",
            "4f3755ab7b974fd68faebad4ff0361a2",
            "704516addfa447c0a789dbab906303e5",
            "1be993ea156842898a11e3af91c2fd6a",
            "7d39aae482074cb1b8d0620f9eedf12d",
            "ade278ffda8c43a5816a2d2f49e1d223",
            "05d8328773764da59c133d385f9f718a",
            "6e735ad18de541c4b9e4487d23957af3",
            "491772c907b3473585d9f8506bf4a2d7",
            "1688f024ab924a6cbf5a745ee15cd039",
            "b625cb6f64c9431fa44c0da156068b9a",
            "3813f5422e814ef8aa6e019cd7ab1487",
            "c23f1a7169de44eaa54e8820fabc5b4b",
            "24533fb3149a410b990543e58b66888a",
            "c4bac6cfb4f74acaa8835eaa688979f9",
            "7b53137d144e4f43b45561aba23a046d",
            "fc85a17da4544c249638c860dde162cf"
          ]
        },
        "id": "HtCdHkOoplDr",
        "outputId": "f936a5ab-1737-42b5-8346-bc3e822d424f"
      },
      "outputs": [],
      "source": [
        "def preprocess_style_text(data):\n",
        "    return bert_tokenizer.batch_encode_plus(\n",
        "            data['text'],\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "style_train_ds_preprocessed = style_train_ds.map(preprocess_style_text, batched=True)\n",
        "style_valid_ds_preprocessed = style_valid_ds.map(preprocess_style_text, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "OpatMrZRdAm_"
      },
      "outputs": [],
      "source": [
        "style_classifier_batch_size = 32\n",
        "style_classifier_num_epochs = 2\n",
        "\n",
        "style_training_args = TrainingArguments(\n",
        "    output_dir=\"bert_shakespeare_style_classifier\",\n",
        "    per_device_train_batch_size=style_classifier_batch_size,\n",
        "    per_device_eval_batch_size=style_classifier_batch_size,\n",
        "    num_train_epochs=style_classifier_num_epochs,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to='none'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "pCblAKhodAqq"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load('accuracy')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "p5wxavbmdAuP"
      },
      "outputs": [],
      "source": [
        "style_trainer = Trainer(\n",
        "    model=bert_style_classifier_model,\n",
        "    args=style_training_args,\n",
        "    train_dataset=style_train_ds_preprocessed,\n",
        "    eval_dataset=style_valid_ds_preprocessed,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "1Zvf_fJ9dAyD",
        "outputId": "ba968a5a-7c9a-481a-959b-34ebc859f62b"
      },
      "outputs": [],
      "source": [
        "style_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "t8huZxEExZnJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Before moving on, save a checkpoint of the model you just trained in your Drive,\n",
        "So that you can pick up where you left off later if needed\n",
        "\"\"\"\n",
        "\n",
        "# Modify this path to the location in your Drive where you want to save the style classifier\n",
        "style_classifier_checkpoint_filepath = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/style_classifier'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "48X7B6HnxaBA"
      },
      "outputs": [],
      "source": [
        "# Run this line only after you've trained the style classifier model\n",
        "bert_style_classifier_model.save_pretrained(style_classifier_checkpoint_filepath, from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "NWOcS_chxaDB"
      },
      "outputs": [],
      "source": [
        "# Run this line only if you need to reload the style classifier you trained earlier\n",
        "bert_style_classifier_model = BertForSequenceClassification.from_pretrained(style_classifier_checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzRAemBTo3Zl"
      },
      "source": [
        "Now let's use the style classifier to classify the output from the Shakespeare translation model, using the test set from our main task. The function reports the average predicted probability of the positive class, which is the modern English style (and which is our goal for our main task model).\n",
        "\n",
        "We should also classify the original Shakespearean text and the human translations from the test set, to compare the scores as references.\n",
        "\n",
        "Run the next two cells of code, then answer the following questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QNYSjDnPLsIy"
      },
      "outputs": [],
      "source": [
        "def get_modern_style_score(text):\n",
        "  text_inputs = bert_tokenizer.batch_encode_plus(\n",
        "            text,\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "  with torch.no_grad():\n",
        "      logits = bert_style_classifier_model.cuda()(text_inputs['input_ids'].cuda(),\n",
        "                                                  attention_mask=text_inputs['attention_mask'].cuda()).logits\n",
        "\n",
        "  probs = softmax(logits.cpu().numpy(), axis=1)\n",
        "  return np.mean(probs[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aylkIJnHo3qE",
        "outputId": "fde62bc4-1c78-4d43-be3a-62bce615e71d"
      },
      "outputs": [],
      "source": [
        "test_original_texts = [task_prefix + pair[0] for pair in test_pairs]\n",
        "test_label_texts = [pair[1] for pair in test_pairs]\n",
        "\n",
        "translations_score = get_modern_style_score(part2_test_translations)\n",
        "reference_score = get_modern_style_score(test_label_texts)\n",
        "shakespeare_score = get_modern_style_score(test_original_texts)\n",
        "\n",
        "print(\"Modern style score for generated translations:  \", translations_score)\n",
        "print(\"Modern style score for reference translations:  \", reference_score)\n",
        "print(\"Modern style score for original Shakespeare:    \", shakespeare_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIwVzWX0MKyc"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 2.e What is the modern style classifier score that you got for the part2 model's generated translations? (Copy and paste the decimal value from the get_modern_style_score function above, to 5 significant digits, e.g. a number like 0.36712 or 0.97632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.f What is the modern style classifier score that you got for the human reference translations? (Copy and paste the decimal value from the get_modern_style_score function above, to 5 significant digits, e.g. a number like 0.36712 or 0.97632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.g What is the modern style classifier score that you got for the original Shakespeare text? (Copy and paste the decimal value from the get_modern_style_score function above, to 5 significant digits, e.g. a number like 0.36712 or 0.97632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.h What do you notice about differences between these scores, and what does that tell you about what the part2 model is doing? (Chose one of the following options that you agree with most, and put it in the answers file.)\n",
        "\n",
        " - A. The part2 model is generating output that is way more modern, casual, and younger generation speak than the human translations.\n",
        "\n",
        " - B. The part2 model is generating output that looks about as modern as the human translations, even if it doesn't always mean the same thing.\n",
        "\n",
        " - C. The part2 model is generating output that is partly modernized, more modern than the original Shakespeare, but still not as modern as the human references.\n",
        "\n",
        " - D. The part2 model is generating output that is still pretty much the same style as the original Shakespeare text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm81djk1OGKM"
      },
      "source": [
        "### 2.5 Revisit Decoder .Generate() Options\n",
        "\n",
        "Now that we have one more evaluation metrics, let's go back to the decoder .generate() arguments we used before. Are there any arguments you want to change, to try to do better on this latest evaluation metric?\n",
        "\n",
        "Try different options for the part2_generate_kwargs below, and run the two cells afterward with each set of choices to see how the evaluation metrics change.\n",
        "\n",
        "Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "rmAE-3c-OGU2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fill in the decoder .generate() arguments that you want to use for the part2 model, like num_beams or top_p, etc.\n",
        "\"\"\"\n",
        "\n",
        "part2_generate_kwargs = {\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLfzUo2nQDot",
        "outputId": "2c5226c4-df21-4788-9355-723ca2b6a0b1"
      },
      "outputs": [],
      "source": [
        "# Print out eval metrics for the part2_model on the test set, with the new kwargs\n",
        "\n",
        "part2_test_translations = calculate_eval_metrics(\n",
        "    test_pairs,\n",
        "    part2_model,\n",
        "    part2_tokenizer,\n",
        "    part2_batch_size,\n",
        "    task_prefix,\n",
        "    **part2_generate_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voOOuiKzQDtY",
        "outputId": "e1c44a88-40dd-4b90-ae07-85e4920af029"
      },
      "outputs": [],
      "source": [
        "# Calculate modern style scores for the part2 translations after using the new kwargs\n",
        "\n",
        "translations_score = get_modern_style_score(part2_test_translations)\n",
        "\n",
        "print(\"Modern style score for generated translations:  \", translations_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOWu7thSHm7L",
        "outputId": "1cfa52b9-027e-43f4-bc9e-44470963e7a1"
      },
      "outputs": [],
      "source": [
        "# Print out a sample of the translated outputs with the revised .generate() parameters\n",
        "\n",
        "for i in range(10):\n",
        "    sample_i = random.choice(range(len(part2_test_translations)))\n",
        "    print('Original:    ', test_pairs[sample_i][0])\n",
        "    print('Reference:   ', test_pairs[sample_i][1])\n",
        "    print('Translation: ', part2_test_translations[sample_i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBlCkKB-QUmy"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 2.i Which decoder strategy seemed to increase the modern style score the most? (Choose one of the following options and put it in the answers file.)\n",
        "\n",
        " - A. Using a stricter option to always choose the highest predicted possibility output (e.g. beam search, or small k or p when using sampling).\n",
        "\n",
        " - B. Using a looser sampling method to allow the model to choose more varied output (e.g. top-k or top-p rather than beam search, especially with higher k or p and/or higher temperature).\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.j What happens to the other evaluation metrics when you try to increase the modern style score by varying the decoder strategy discussed in 2.i? (Choose one of the following options and put it in the answers file.)\n",
        "\n",
        " - A. BLEU and BLEURT both seem to be positively correlated with the modern style score, when changing the decoder strategy.\n",
        "\n",
        " - B. BLEU and BLEURT both seem to be negatively correlated with the modern style score, when changing the decoder strategy.\n",
        "\n",
        " - C. BLEU seems to move with the modern style score, but BLEURT seems to go the other direction.\n",
        "\n",
        " - D. BLEURT seems to move with the modern style score, but BLEU seems to go the other direction.\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 2.k Why do you think the relationship in question 2.j is happening? (Choose one of the following options and put it in the answers file.)\n",
        "\n",
        " - A. A stricter decoder strategy makes the model more likely to output the best translation, which is good for BLEU, BLEURT, and modern style objectives.\n",
        "\n",
        " - B. A looser decoder strategy gives the model more freedom to find a good modern style translation, which should also end up saying more of the same things in the same way as the human translation.\n",
        "\n",
        " - C. A stricter decoder strategy makes the model more likely to output a translation that has correct exact words and style, increasing BLEU and modern style scores, but might not mean the same thing as the human translation.\n",
        "\n",
        " - D. A looser decoder strategy gives the model more freedom to choose more modern style words, which the pre-trained model is more familiar with, but that freedom can make the model less likely to end up with the exact same words or meaning as the human translation.\n",
        "\n",
        " - E. A stricter decoder strategy makes the model more likely to choose more of the exact same words as used in the dataset, but not necessarily in the same order, so the meaning and style don't end up being as close to the human translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl5_wE3CJ02W"
      },
      "source": [
        "## 3. Adding Supplementary Paraphrase Dataset\n",
        "\n",
        "Can we do anything else to make the model capable of rephrasing the input text more into a different style (i.e. modernizing it more fully away from the Shakespeare), but still keep the same meaning?\n",
        "\n",
        "One related task that could help is a paraphrasing task. The [GLUE Microsoft Research Paraphrase Corpus (MRPC)](https://huggingface.co/datasets/nyu-mll/glue) dataset has pairs of sentences with labels indicating whether the two sentences are equivalent (i.e. they mean the same thing) or not.\n",
        "\n",
        "We could use that data as an additional supporting task for our T5 model, to see if it helps our model get better at accurately rephrasing the input text into a differently worded output.\n",
        "\n",
        "### 3.1 Load and preprocess the supplemental dataset\n",
        "\n",
        "Load the dataset from Huggingface and look at the contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "8c9123e1b15245d5963dc37dfe4d1184",
            "affbc6ae9eea47a89b1aeb9d57d25465",
            "f44c4d07570840ff9cee7d152fa19eaa",
            "1c5afa2b0fec4f01b87385c704fe255c",
            "d6787c72194f4ba5adf7dce0c2e53369",
            "5140af3e25a446aab189b14f3c2b8d72",
            "b773928516b846c48420655fb744b9a8",
            "ac7a766e9fea4884a4c2ff30aa5f714e",
            "dad7317cc6b74923ac0796651be8d8ff",
            "ea69283da17b4c1ba673eb6a33531ebc",
            "2ebec71d244c4087a646ab2eeae35ce1",
            "7c2cd08c914545308aeb5c4a7060fc44",
            "33f138d8719b414085a7886996c96db0",
            "30180bca424043d1ac06dcfd14a4ee44",
            "428f5ed58773419fad6087885d31752d",
            "4e2035960efa4f75b488420a999474ad",
            "7409b515a058431086d10f507fa2bfaf",
            "a757c471bbda4f3880cd10a8e1d71692",
            "430bc347d5ff4905b00d529e7a51d37d",
            "6092edc857204c898d2c1caa89470034",
            "b82e360549924b97962775ca83ce0435",
            "b0460e4743b14949a7c53a4af6fc4103",
            "f6aab693a7994d3ea47aa20a7d9efde8",
            "15ea822affb64940b07afac635c628a1",
            "0a579c8148864f219850ac3fe01fe9a8",
            "dbc6967a26ce4dc581250b47956d57a0",
            "7465ae58363f46de91cde794d570def4",
            "47180a3bfaff4cd8b8a5de2bbadf40cb",
            "f9095af8c2f34e6999ca55ebac671129",
            "c6e1992318364667ad1879369a9e0825",
            "175822b5ee0042fd8c4edb5340668f94",
            "9caf4329484f4426be1ea3b3c9f5fe70",
            "eca57bdc60124dd380209da2c992af0f",
            "651437d0c3624d41ade7c5b8edf01e1b",
            "b1e0b8591bb746b1aab322526af184cf",
            "1b328b2f26a34d13b7ca3ad757e9ca17",
            "952b7e2d4e2e476f8f9eab819a93f653",
            "65f4b5edd71741fc9cb3cc07f781794f",
            "88bb7975735f4e74a726f98e932d7095",
            "a00f232b3cd54c2e9a3e24abcf268595",
            "6bfd306ad67f47888def353cd2897246",
            "27675b03c30645b8ac2f7538dfeefc67",
            "197b1a6ed2304933b6686df9e13fd47e",
            "14cd7dca5a734d00b20f018baad7670c",
            "28898c12a1a2445a896b6ab19cbed2c4",
            "199025b79ebb4f7e92fba3e818aea2a0",
            "cc8c115ff87c4c1bb79f5f0293b6268e",
            "26ffd7c43c144651abf30226ad243b52",
            "d9dc057f5d1a4778887dc6b3c7ea619f",
            "4bdc5a23c0574ec0bf8aba6777ad82d7",
            "4fb2942c46f34bba8e01313c3fb32207",
            "b49ab209a9ff46749a4614c4f3cc6db3",
            "81063ae583be4220a7085fd933230647",
            "28794c300efc43dda040209a7ce15780",
            "a0138e9644d9419585ea02a84983b3a4",
            "37d8c622475148ce951493ff9f92497e",
            "ef1460f7420d48ed9bf74c25331577b7",
            "10a581249771456e8dd354ec30a19940",
            "1cf064f214ab432fbca4ad841a599a7b",
            "df9d5484f952410caf3e8f70afae13e7",
            "90160afa28d049f7bf61232ef5667683",
            "96a2c67deaa6489898d58651bc8ae7df",
            "4563a3bfe5974f4ca2c98ac57c183d06",
            "3fbc09bac3bd4a07a0f6e81527f5512b",
            "9d8a4b0336af48e5a3dee59b5912e8fe",
            "7c9f108990cd4c0bab7146f8357821ea",
            "62406640c73847bc9d2056b2c4df56b6",
            "a32d30486a4f42df8e407634945ac875",
            "33ab4493ebdf4a83a5d03e045d4b8383",
            "82435f6fe4f543eb94ff116f82b86dcc",
            "ba3504f88b5f4893bddd4328079ca3e1",
            "d40c0b2326ec4f55a73ffee48e6b0585",
            "171c506ec1f14f6891dfd1b6887381c8",
            "4176cda1965c456288d13b8c6cb97a08",
            "e4fe986cba1b4ac19d0bfaa71c26b9ee",
            "edd1a74b39aa4952b1f74af5fa218883",
            "5fa0561c55be4b258d76d8271047a6b8"
          ]
        },
        "id": "sqdtBmtpJ1Jf",
        "outputId": "a7cba485-cb70-46ca-a628-57fc49823f8c"
      },
      "outputs": [],
      "source": [
        "mrpc_data = load_dataset('SetFit/mrpc', trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ2QDvSdJ1Mc",
        "outputId": "67eae692-eb50-4e5d-b289-08da16956e38"
      },
      "outputs": [],
      "source": [
        "mrpc_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YfmyHCsJ1PW",
        "outputId": "4d418daa-eb15-4e5f-ed27-5c524f65f765"
      },
      "outputs": [],
      "source": [
        "\n",
        "mrpc_data['train'].features['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjuJt3jdJ9PT"
      },
      "source": [
        "Let's use just the pairs that are labeled as equivalent (correct paraphrases), and split the sentences to use the first as the model's input and second as the model's output. Then we can use that to train our T5 model to better generate rephrased statements in modern English with the same meaning as the input but in different words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "498d507e409945f0b84edcd0b5c7d0b3",
            "04dbd50783d34480b5764d2f5f12c046",
            "64beb952df514db783fadea4e39b528c",
            "3ec947193fc84d69960c42c936420296",
            "156fc5f72f80495cad268e8b62765b24",
            "f6f657fc62514bd8bfd2de00eaeb50a7",
            "b9eb2a5713e247aaba254ea60abf17fa",
            "fd3b89418de346e78cc4d74e060c8ac0",
            "4439d9610e294efeadd7efe4645a7b97",
            "5f36654e60bd423a8b3eedb816e34687",
            "6bacc7437aa647f7acad712d0a656ae8",
            "0ad283b0eef84aa3bd36fbb019a39d83",
            "22cb94ffef4544d9b5d2049d71bcccfd",
            "38995a51b9b749f084808acc576f7bf8",
            "fcaa64ffe2164da3aa6bf27e9907b775",
            "fb123e904bbc4fd3b6a8a4f6c55e2ffe",
            "e73af85401254fed96155e3cace2000c",
            "56af1227fa2d4ffa8eb6ff77b8a3b557",
            "df20b0bb14a04e18bec7c5ded09bd5ab",
            "0936ac22a46c4886abe67294325ae225",
            "1d16d58629244087bd919e9c6d11754a",
            "ae212ff22dfd4e39bb867af9338d4ecc"
          ]
        },
        "id": "L1F2fPeYJ1SE",
        "outputId": "4247b4cd-9515-465f-ec6f-35fb7fd076fd"
      },
      "outputs": [],
      "source": [
        "mrpc_equiv_train = mrpc_data['train'].filter(lambda example: example['label'] == 1)\n",
        "mrpc_equiv_valid = mrpc_data['validation'].filter(lambda example: example['label'] == 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVBfoSwKN4L_"
      },
      "source": [
        "Fill in the code below to encode \"sentence1\" as the model's input and \"sentence2\" as the model's output.\n",
        "\n",
        "We will also add a different prefix for this supporting task, so make sure to add the prefix to the inputs in the function below. (You can use the preprocess_translation_batch function above as an example.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "BtmZkZkudA80"
      },
      "outputs": [],
      "source": [
        "def preprocess_mrpc_for_paraphrase_generation(mrpc_ds, tokenizer, prefix):\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    # if prefix: ...\n",
        "    # input_encoded = ...\n",
        "    # output_encoded = ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return {'input_ids': input_encoded['input_ids'],\n",
        "            'labels': output_encoded['input_ids']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UksD44fPkn6"
      },
      "source": [
        "Now map the preprocessing function to the MRPC train and validation datasets. Use the part2_tokenizer to preprocess the data, since we're using the same T5 pre-trained model checkpoint as in part 2. For the preprocessing function's \"prefix\" argument, use the paraphrase_prefix provided below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d55859385e174503964374034f37f064",
            "8ee51fb51d5d4acf8f21c879270a1550",
            "7c4b69e964de4d4080b81bee8f16dd97",
            "771ac2185bfd4c0a82538aab89c8e90f",
            "1020d0789cad447d9678c5a363976061",
            "4613e95a5d3344b6bdf69cb3c957ec99",
            "2e7423fdef0544699ffe0af6d9d0c688",
            "e79c88fc3f7a4edeaaacaa32ea2b7113",
            "f07781ea45814691837aec1cf3ef1472",
            "53a36e46d85e43f18eea8baede9acb63",
            "dddf9d69b01940c58af868a6f46359cd",
            "38efec9b8a304af3a0dfbe3c083ef28f",
            "ed58740287ea43efa19e69bd7b1320d7",
            "0b117c716cb0441d9f68494f7ec2b389",
            "ece63ceabd1841dc82f94b2fd0e23ce3",
            "42cd49dbc02c40c38a31b71101db897e",
            "dec4110067724e39ab725ebcb17630d3",
            "119743e926234efcae23ac4d51c0437a",
            "6b3679f63ac341b29a99f3afd60bcf1f",
            "bcb1c804cac44a08b88ddc697962c6b1",
            "c1207991605f4fa1be79eb0176022fd3",
            "2b558ffc46464f2aa4dd6642cf496ef1"
          ]
        },
        "id": "CgB2B-P6dBAV",
        "outputId": "5fe7f4cd-8991-4df3-bccf-376d6532809c"
      },
      "outputs": [],
      "source": [
        "paraphrase_prefix = 'Paraphrase in modern English: '\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# mrpc_paraphrase_train = ...\n",
        "# mrpc_paraphrase_valid = ...\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba4cLyiyQMMJ"
      },
      "source": [
        "### 3.2 Train T5 on Paraphrasing Task\n",
        "\n",
        "Load a fresh copy of the pre-trained T5 model (using the same pre-trained model checkpoint as part2), so that we can train it first on the paraphrase task, and last on the main task data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "_Tgg2lBWQWDV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Load a new copy of the same pre-trained model (we'll use the same in tokenizer as part2)\n",
        "\"\"\"\n",
        "\n",
        "t5_pretrained_checkpoint_name = 'google-t5/t5-small'\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# part3_model = ...\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs4Uy482Q5V-"
      },
      "source": [
        "Now create the training args and trainer for the paraphrase task, and train the model. Use the `create_seq2seq_training_args` and `create_seq2seq_trainer` functions like before.\n",
        "\n",
        "You'll be using the part3_model you just loaded, and the MRPC data you preprocessed. Use the batch_size and num_epochs provided for the paraphrase task below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "FraCXsP6Rcr-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create the training args and trainer for the paraphrase task.\n",
        "\"\"\"\n",
        "\n",
        "paraphrase_batch_size = 32\n",
        "paraphrase_num_epochs = 4\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# paraphrase_training_args = ...\n",
        "# paraphrase_trainer = ...\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "l_Adc3AnL_1X",
        "outputId": "0cc57b69-0f72-4eb8-c765-b164d9672364"
      },
      "outputs": [],
      "source": [
        "paraphrase_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkIvMxIeU0jz"
      },
      "source": [
        "### 3.3 Fine-Tune Paraphrase-Trained Model on Main Task\n",
        "\n",
        "Now create the training args and trainer for the main task. Use the `create_seq2seq_training_args` and `create_seq2seq_trainer` functions one more time.\n",
        "\n",
        "You'll be using the same model that you just trained on the paraphrase task (part3_model). Use the batch size and num epochs provided below.\n",
        "\n",
        "For training data, use the same data as part2: `train_ds_part2` and `val_ds_part2`. We're using the same pre-trained model checkpoint, i.e. the same tokenizer, and the same task prefix, so the data has already been preprocessed correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "MScmxiy9ZaIO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create the training args and trainer for the main task using the part3_model.\n",
        "\"\"\"\n",
        "\n",
        "part3_batch_size = 32\n",
        "part3_num_epochs = 4\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "# part3_training_args = ...\n",
        "# part3_trainer = ...\n",
        "\n",
        "\n",
        "\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "FV1aOzpXcI1v",
        "outputId": "1223d362-f5ac-45b6-84c1-fa61080f4629"
      },
      "outputs": [],
      "source": [
        "part3_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "di4zMzWtx95k"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Before moving on, save a checkpoint of the model you just trained in your Drive,\n",
        "So that you can pick up where you left off later if needed\n",
        "\"\"\"\n",
        "\n",
        "# Modify this path to the location in your Drive where you want to save the part3 model\n",
        "part3_model_checkpoint_filepath = 'drive/MyDrive/ISchool/MIDS/266/model_checkpoints/part3_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "hTi-rOy9x-DS"
      },
      "outputs": [],
      "source": [
        "# Run this line only after you've trained the part3 model on both tasks\n",
        "part3_model.save_pretrained(part3_model_checkpoint_filepath, from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "VgzXnWk1x-F0"
      },
      "outputs": [],
      "source": [
        "# Run this line only if you need to reload the part3 model you trained earlier\n",
        "part3_model = T5ForConditionalGeneration.from_pretrained(part3_model_checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJqR5W1hZyUs"
      },
      "source": [
        "### 3.4 Paraphrase-Trained Model Evaluation\n",
        "\n",
        "Use the functions defined above to translate the test set and calculate the same set of evaluation metrics as used on the part2 model.\n",
        "\n",
        "Use the same decoder .generate() arguments as part2 (`part2_generate_kwargs`), so that we can compare the part2 and part3 models as closely as possible.\n",
        "\n",
        "Run the next three cells, then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pXcdJGAL_58",
        "outputId": "33ea8d00-9722-41b3-bdf2-7e8f95eddc96"
      },
      "outputs": [],
      "source": [
        "# Print out eval metrics for the part3_model on the test set, with the new kwargs\n",
        "\n",
        "part3_test_translations = calculate_eval_metrics(\n",
        "    test_pairs,\n",
        "    part3_model,\n",
        "    part2_tokenizer,\n",
        "    part3_batch_size,\n",
        "    task_prefix,\n",
        "    **part2_generate_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAw04aBVMxBY",
        "outputId": "0a7f07cb-037d-4f97-e605-41426ab7c90f"
      },
      "outputs": [],
      "source": [
        "# Calculate modern style scores for the part3 translations after using the new kwargs\n",
        "\n",
        "translations_score = get_modern_style_score(part3_test_translations)\n",
        "\n",
        "print(\"Modern style score for generated translations:  \", translations_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwetEG0oMyNh",
        "outputId": "8eb800d4-48f6-4e25-ff56-ecf481f6c1a6"
      },
      "outputs": [],
      "source": [
        "# Print out a sample of the translated outputs to look at as well\n",
        "\n",
        "for i in range(10):\n",
        "    sample_i = random.choice(range(len(part3_test_translations)))\n",
        "    print('Original:    ', test_pairs[sample_i][0])\n",
        "    print('Reference:   ', test_pairs[sample_i][1])\n",
        "    print('Translation: ', part3_test_translations[sample_i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb8p115NaORe"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        " 3.a What is the overall BLEU score that you achieved on the test set for the part3 model? (Copy and paste the decimal value for the overall BLEU score, to 5 significant digits, e.g. a number like 0.03671 or 0.09763. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 3.b What is the mean BLEURT score that you achieved on the test set for the part3 model? (Copy and paste the decimal value for the mean BLEURT score, to 5 significant digits, e.g. a number like -1.12345 or -0.54321. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 3.c What is the modern style classifier score that you got for the part3 model's generated translations? (Copy and paste the decimal value from the get_modern_style_score function above, to 5 significant digits, e.g. a number like 0.36712 or 0.97632. Put the answer in the answers file; it should match the value shown in your output in this notebook.)\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        " 3.d How do the part3 model's evaluation scores and output compare to the part2 model? Write a short answer about what you observe in the markdown cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AulNHRSbIc5"
      },
      "source": [
        "*** YOUR ANSWER TO QUESTION 3.d HERE IN THIS TEXT CELL***\n",
        "\n",
        "*** END YOUR ANSWER ***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJRE6UeaL_9m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzmSsltb01Vh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
